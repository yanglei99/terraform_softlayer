scontrol show nodes
sinfo
scontrol show partition

srun -N2 -l /bin/hostname
srun -n4 -l /bin/hostname

cat > my.script << FIN
#!/bin/sh
#SBATCH --time=1
/bin/hostname
srun -l /bin/hostname
srun -l /bin/pwd
FIN

sbatch -n4 -w "myslurm-worker-[00-01]" -o my.stdout my.script
scontrol show job 3
squeue

cat > my_dep.script << FIN
#!/bin/sh
#SBATCH --time=1
/bin/hostname
srun -l /bin/hostname
srun -l /bin/pwd
[root@myslurm-master-00 ~]# cat my1.script 
#!/bin/bash
#SBATCH --job-name="atest"
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --time=00:02:00
#SBATCH -o stdout.%j
#SBATCH -e stderr.%j
#SBATCH --export=ALL

#----------------------
cd $SLURM_SUBMIT_DIR
date
srun -n 2 sleep 30
date

ls this_file_does_not_exist


srun -n 2 hostname -s
FIN


sbatch -p debug my_dep.script
sbatch --dependency=afterany:4 -p debug my_dep.script
sbatch --dependency=afterany:5 -p debug my_dep.script
squeue
scontrol show job 5


==============GPU ====================

cat > gres_test.sh << FIN
#!/bin/bash
#
# gres_test.sh
# Submit as follows:
# sbatch --gres=gpu:4 -n4 -N1-1 gres_test.sh
#
srun --gres=gpu:2 -n2 --exclusive  hostname -s &
srun --gres=gpu:1 -n1 --exclusive  hostname -s &
srun --gres=gpu:1 -n1 --exclusive  hostname -s &
wait

FIN

squeue
scontrol show job 6
